{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fedab54-defb-47ed-ae6e-ca4fb48ece7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.0.2\n",
      "  Downloading transformers-3.0.2-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.9/44.9 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from transformers==3.0.2) (1.26.4)\n",
      "Collecting tokenizers==0.8.1.rc1 (from transformers==3.0.2)\n",
      "  Downloading tokenizers-0.8.1rc1.tar.gz (97 kB)\n",
      "     ---------------------------------------- 0.0/97.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 97.4/97.4 kB 2.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from transformers==3.0.2) (24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from transformers==3.0.2) (3.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from transformers==3.0.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from transformers==3.0.2) (4.66.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from transformers==3.0.2) (2022.10.31)\n",
      "Collecting sentencepiece!=0.1.92 (from transformers==3.0.2)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting sacremoses (from transformers==3.0.2)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers==3.0.2) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers==3.0.2) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers==3.0.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->transformers==3.0.2) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests->transformers==3.0.2) (2022.12.7)\n",
      "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from sacremoses->transformers==3.0.2) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from sacremoses->transformers==3.0.2) (1.2.0)\n",
      "Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "   ---------------------------------------- 0.0/769.0 kB ? eta -:--:--\n",
      "   ----------------------- --------------- 471.0/769.0 kB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 769.0/769.0 kB 9.7 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   --------------------------------------  983.0/992.0 kB 31.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 992.0/992.0 kB 20.9 MB/s eta 0:00:00\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   --------------------------------------  890.9/897.5 kB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 897.5/897.5 kB 18.9 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [46 lines of output]\n",
      "  C:\\Users\\pgarl\\AppData\\Local\\Temp\\pip-build-env-q9qc_4mq\\overlay\\Lib\\site-packages\\setuptools\\dist.py:330: InformationOnly: Normalizing '0.8.1.rc1' to '0.8.1rc1'\n",
      "    self.metadata.version = self._normalize_version(self.metadata.version)\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-312\\tokenizers\n",
      "  copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-312\\tokenizers\\models\n",
      "  copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-312\\tokenizers\\decoders\n",
      "  copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-312\\tokenizers\\normalizers\n",
      "  copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-312\\tokenizers\\pre_tokenizers\n",
      "  copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-312\\tokenizers\\processors\n",
      "  copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-312\\tokenizers\\trainers\n",
      "  copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "  copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\n",
      "  copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\models\n",
      "  copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\decoders\n",
      "  copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\normalizers\n",
      "  copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\pre_tokenizers\n",
      "  copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\processors\n",
      "  copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\trainers\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==3.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b62a59-cc6b-4efd-9d26-f29f9a666aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_hub\n",
      "  Downloading keras_hub-0.19.2-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting keras>=3.5 (from keras_hub)\n",
      "  Downloading keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: absl-py in c:\\python312\\lib\\site-packages (from keras_hub) (1.4.0)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from keras_hub) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from keras_hub) (24.0)\n",
      "Requirement already satisfied: regex in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from keras_hub) (2022.10.31)\n",
      "Requirement already satisfied: rich in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from keras_hub) (12.5.1)\n",
      "Requirement already satisfied: kagglehub in c:\\python312\\lib\\site-packages (from keras_hub) (0.2.7)\n",
      "Requirement already satisfied: namex in c:\\python312\\lib\\site-packages (from keras>=3.5->keras_hub) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\python312\\lib\\site-packages (from keras>=3.5->keras_hub) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\python312\\lib\\site-packages (from keras>=3.5->keras_hub) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\python312\\lib\\site-packages (from keras>=3.5->keras_hub) (0.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from kagglehub->keras_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from kagglehub->keras_hub) (4.66.4)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\python312\\lib\\site-packages (from rich->keras_hub) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras_hub) (2.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\python312\\lib\\site-packages (from optree->keras>=3.5->keras_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from requests->kagglehub->keras_hub) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from requests->kagglehub->keras_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->kagglehub->keras_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests->kagglehub->keras_hub) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\pgarl\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->kagglehub->keras_hub) (0.4.6)\n",
      "Downloading keras_hub-0.19.2-py3-none-any.whl (705 kB)\n",
      "   ---------------------------------------- 0.0/705.1 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 102.4/705.1 kB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 655.4/705.1 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 705.1/705.1 kB 7.4 MB/s eta 0:00:00\n",
      "Downloading keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 0.8/1.3 MB 24.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 14.2 MB/s eta 0:00:00\n",
      "Installing collected packages: keras, keras_hub\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.4.1\n",
      "    Uninstalling keras-3.4.1:\n",
      "      Successfully uninstalled keras-3.4.1\n",
      "Successfully installed keras-3.9.0 keras_hub-0.19.2\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "764d27f0-e7af-448a-849f-94af59442352",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mERROR)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_hub\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras_hub'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm #\n",
    "from sklearn import metrics\n",
    "import transformers \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import keras_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11c70ccd-6a89-46d4-a937-d27a0fef41d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'disable_warnings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdisable_warnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'disable_warnings'"
     ]
    }
   ],
   "source": [
    "tokenizer = keras_hub.models.DistilBertTokenizer.from_preset(\n",
    "    \"distil_bert_base_en_uncased\",\n",
    ")\n",
    "tokenizer(\"The quick brown fox jumped.\")\n",
    "\n",
    "# Batched input.\n",
    "tokenizer([\"The quick brown fox jumped.\", \"The fox slept.\"])\n",
    "\n",
    "# Detokenization.\n",
    "tokenizer.detokenize(tokenizer(\"The quick brown fox jumped.\"))\n",
    "\n",
    "# Custom vocabulary.\n",
    "vocab = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "vocab += [\"The\", \"quick\", \"brown\", \"fox\", \"jumped\", \".\"]\n",
    "tokenizer = keras_hub.models.DistilBertTokenizer(vocabulary=vocab)\n",
    "tokenizer(\"The quick brown fox jumped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be99e61d-e6e9-4c72-a034-125d9de1ac5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras_hub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m keras_hub\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mDistilBertTokenizer\u001b[38;5;241m.\u001b[39mfrom_preset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistil_bert_base_en_cased\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras_hub' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = keras_hub.models.DistilBertTokenizer.from_preset(\"distil_bert_base_en_cased\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b7f8247-bedd-4716-8285-8be2d339a3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "No CUDA device found\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_name(0) if torch.cuda.is_available() else \"No CUDA device found\")\n",
    "#expected. will need to discuss with team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9af501c0-45ba-4e28-94b7-2cce307d0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set(np.where(y_true[i])[0])\n",
    "        set_pred = set(np.where(y_pred[i])[0])\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/ float(len(set_true.union(set_pred)))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573aeda-0a88-41b5-b91a-197039b44034",
   "metadata": {},
   "source": [
    "data = pd. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867628b-0ade-4201-9549-5f034c9a1daf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c735f85-92b4-462b-9e59-ae161de6ccff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e55905-97a3-4dba-864f-60b8d864c9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec301ce-4b1f-463b-9458-48c9feef5b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf6450-f349-4729-a515-b3739642bd76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3bcb31-af5e-4f30-939a-1ced92ab2397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4abe7b-2630-48ac-8961-b0831285b943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c97f7-494f-489e-a08c-78a42943e6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d21f03-4a12-4fbb-ac59-e785b1d7816c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691ea7c-1514-4b4d-8096-d7167e8ad236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
